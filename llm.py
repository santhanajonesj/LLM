# -*- coding: utf-8 -*-
"""LLM_02(17_11_2024).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kcdyKm4A1CviInuo1F17RNF3BbF7ja8p
"""

model_file = '/content/drive/MyDrive/llm/llama-2-7b-chat.ggmlv3.q8_0.bin'

!pip install langchain

!pip install sentence-transformers

!pip install langchain_community

!pip install ctransformers

!pip install faiss-gpu

pip install pypdf



from langchain.document_loaders import PyPDFLoader, DirectoryLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

from langchain.vectorstores import FAISS

import box
from langchain.llms import CTransformers

dir_files = DirectoryLoader("/content/drive/MyDrive/llm", glob= "*.pdf", loader_cls= PyPDFLoader)

docs = dir_files.load()

splited_text = RecursiveCharacterTextSplitter(chunk_size = 550, chunk_overlap = 50)

documents_splitted = splited_text.split_documents(docs)

documents_splitted

embedding = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2",
                                  model_kwargs = {"device": "cpu"})

vect = FAISS.from_documents(documents_splitted, embedding)

vect.save_local("db/faiss")

llm_model = CTransformers(
    model = model_file,
    model_type = "llama",
    config = {"max_new_tokens":500, "temperature": 0.01}
)

# A low temperature (close to 0) makes the output more deterministic (less creative).
# A high temperature (e.g., 1.0) increases randomness, leading to more diverse outputs.

# prompt engineering

it is the process of designing and refining prompts effectively interact with language models(like gpt,LLama etc)

How you ask : A well designed prompt can drastically improve response qulity
what you include :provide clear instructions and relevent context for better o/p

"""
you are a medical chatbot /
you should respond with some medications or tablets /

if you do not know the answer, do not makeover, Just respond as 'No Idea' /

Example: what is that tablet name?
        bot answer : <tablet name>

"""

prompt = """

use the following information and answer the question based on that /
provide only useful informations /
if you do not know, do not try to make wrong answers /
if you do not know, just respond as 'I do not know' /

show only the answer alone /
Example: what is the name in doc?
       output: Raj

Context: {context}
Questions: {question}


"""

# making prompt template

from langchain import PromptTemplate
from langchain.chains import RetrievalQA

# creating sub functions

def sqa_prompt():
  prompt_temp = PromptTemplate(template = prompt, input_variables = ["context", "question"])
  return prompt_temp

def ret_answer(llm_model_name,prompt_name,v_db):
  ques_ans = RetrievalQA.from_chain_type(
      llm = llm_model_name,
      chain_type = "stuff",
      chain_type_kwargs = {"prompt": prompt_name},
      retriever = v_db.as_retriever(search_kwargs={'k':2})
  )
  return ques_ans

#"stuff": Concatenates all retrieved documents into a single input for the language model.

def setup_mode():
  embb = HuggingFaceEmbeddings(model_name = "sentence-transformers/all-MiniLM-L6-v2",
                                  model_kwargs = {"device": "cpu"})
  vdb = FAISS.load_local("/content/db/faiss", embb, allow_dangerous_deserialization= True)

  pmt = sqa_prompt()

  get_answer = ret_answer(llm_model, pmt, vdb)
  return get_answer

# final load run of llm model

llm_qa_model = setup_mode()

asked_question = 'What is account number?'

llm_qa_model({'query':asked_question})

